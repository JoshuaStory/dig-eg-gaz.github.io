<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/xslt/rss.xslt" ?>
<?xml-stylesheet type="text/css" href="/assets/css/rss.css" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Digital Egyptian Gazette</title>
		<description>A daily newspaper from turn-of-the-century Egypt, in marked up full text.</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>“Difficulties with OCR”</title>
				<link>/technical/black-difficulties-with-OCR/</link>
				<pubDate>Wed, 09 Nov 2016 00:00:00 -0500</pubDate>
				<description>&lt;p&gt;One of the main goals in this class is to covert an old newspaper into a website that allows it to be easily accessed by anyone. In order to do this we must complete one very vital, and apparently very difficult step: covert pictures of microfilm into reader editable, text. This is called OCR (optical character recognition) and thankfully there are lots of programs available to us to make this process extremely simple, or so it’s thought. One of these programs is ABBYY’s FineReader, which was the one first suggested to us. This program apparently does wonders if you are using a Windows computer, but for us Mac users it only creates disappointment. Instead of converting our images into readable text, it converts them into an agglomeration of random characters that sometimes resemble words in the English language. I’ve provided a short example of one of these “conversions” done on the first few lines of one of my microfilm images. I’ve tried rescanning my images, making adjustments in FineReader, and tried a different OCR program to try and achieve better results and nothing has worked in my favor.  So, instead of making small corrections to the text as it should be, I am forced to basically retype the entire page. This process is &lt;strong&gt;extremely time-consuming&lt;/strong&gt; and I am positive that if I had an OCR program that loved my Mac computer as much as others seem to love Windows ones, I would have completed much more of my work by now.&lt;/p&gt;
</description>
				<guid isPermaLink="true">/technical/black-difficulties-with-OCR/</guid>
			</item>
		
			<item>
				<title>OCR OCSucks</title>
				<link>/technical/Sain-technical-reflection/</link>
				<pubDate>Wed, 09 Nov 2016 00:00:00 -0500</pubDate>
				<description>&lt;p&gt;Although the whole process of learning new word processing and encoding programs was challenging, the most frustrating aspect was the &lt;a href=&quot;https://www.abbyy.com/en-us/finereader/about-ocr/what-is-ocr/&quot;&gt;OCR&lt;/a&gt;. It is supposed to be a faster way to translate the newspaper pages into plain text, it turned out to take up about as much time as it would have to hand type it all. I felt as if no matter how many times I scanned the &lt;a href=&quot;https://www.nedcc.org/free-resources/preservation-leaflets/6.-reformatting/6.1-microfilm-and-microfiche&quot;&gt;microfilm&lt;/a&gt; images and changed the focus, brightness, contrast, or zoomed in on the page, it would not run through &lt;a href=&quot;https://www.abbyy.com/en-us/&quot;&gt;ABBYY&lt;/a&gt; in the way I was expecting it to. I was hoping to have to make minimal corrections after the scanned images were OCRed, but with all the pages I have done up to this point, I have had to make a minimum of two hours’ worth of corrections per page. The word processing software may read a majority of the words (I would estimate around 85%), I still spent a majority of my time in this class fixing the mistakes the OCR made. I also found the other programs in this class easier to use than ABBYY. Sometimes while editing a word, the image it received it from would disappear, I also found the ordering of the paragraphs challenging at times. Overall, the program may have saved me some time, but another OCR processor may be more efficient, from my experience.&lt;/p&gt;
</description>
				<guid isPermaLink="true">/technical/Sain-technical-reflection/</guid>
			</item>
		
			<item>
				<title>Running out of Time</title>
				<link>/technical/Taylor-blog-post-2/</link>
				<pubDate>Tue, 08 Nov 2016 00:00:00 -0500</pubDate>
				<description>&lt;p&gt;Converting images to text has been a difficult process from start to finish. From the very start, FineReader did not seem to be responding well to my scans. It produced a multitude of errors (nearly every other word at points) that did not seem to be the result of poor scans. Then, one day I went to do some more work with it, and the program wouldn’t start up even though my trial period was not over yet. It displayed a message which said that ABBYY FineReader 12 would be terminated because it was corrupted. I discovered that one other student had the exact same problem, so at least I knew that it was not just my little computer giving up on life from working so hard.&lt;/p&gt;

&lt;p&gt;Thankfully I only had a couple more page threes to do at that point. So, I used my roommate’s Mac and did those. Now that I’ve moved on to other pages, I’m just typing up whatever articles appear since this doesn’t seem to take too much longer than it did to go back and correct all the text. All in all, this class has been far more difficult than I envisioned it being (especially for someone who is not great with computers), but I am not ready to give up on it.&lt;/p&gt;
</description>
				<guid isPermaLink="true">/technical/Taylor-blog-post-2/</guid>
			</item>
		
			<item>
				<title>Time Issues</title>
				<link>/technical/hofmeister-time-issues/</link>
				<pubDate>Mon, 07 Nov 2016 00:00:00 -0500</pubDate>
				<description>&lt;p&gt;To be honest, this class is not what I expected it to be. I did not expect
to be learning how to code things or doing xml or anything that involved
technical skills. While learning the skills is not difficult, it is
difficult to find the time it takes to complete the assignments.
I try to set aside time to complete pages and to edit the text,
however I can never find enough time. For me, this is a huge issue
because I am super obsessive about my grades and I am actually
concerned about not receiving a good grade in this class. I do not face
any technical issues, just time issues. Thus far I have completed the first
three pages of everyday , the only issue in the pages that I find is not
being able to read the data for financial tables. The data is always written
very small and it tends to be smudged and &lt;strong&gt;I hate it and it gives me a headache.&lt;/strong&gt;&lt;/p&gt;
</description>
				<guid isPermaLink="true">/technical/hofmeister-time-issues/</guid>
			</item>
		
			<item>
				<title>When a Patch Won&#39;t Do!</title>
				<link>/technical/-/curiosities/cyr-WhenAPatchWontDo/</link>
				<pubDate>Sun, 06 Nov 2016 00:00:00 -0400</pubDate>
				<description>&lt;p&gt;Attempting to digitize a year of the Egyptian Gazette newspaper proves difficult as students find their individual difficulties in attempting the task.&lt;/p&gt;

&lt;p&gt;The year to be digitized is 1905, and each student has been given a week to digitize and examine the newspaper’s contents as the task is completed. Getting the pages from the microfilm to a more reader friendly medium is a task for a patient soul!&lt;/p&gt;

&lt;p&gt;One problem encountered is a large hole that the &lt;em&gt;original&lt;/em&gt; print newspaper had just before it was scanned. Information lost at this stage is detrimental as there are only two copies of microfilm in the country (both copies are most likely identical copies) and a trip to London to see the original remaining Egyptian Gazette copies was, unfortunately, declined by our instructor!&lt;/p&gt;

&lt;p&gt;An important lesson has been learned in preserving past information when possible. Microhistorical details can be lost, when in some rare instances, one bit of information is all it takes to piece together a puzzle that has plagued historians since the time the articles have been written.&lt;/p&gt;

&lt;p&gt;The result is simply accepting our realities and a great lesson towards preservation.&lt;/p&gt;

&lt;h2 id=&quot;for-further-study&quot;&gt;For further study:&lt;/h2&gt;
&lt;p&gt;Best Way to Preserve Your Newspaper:
&lt;a href=&quot;http://www.mnhs.org/preserve/conservation/reports/nytimes_preserving.pdf&quot;&gt;http://www.mnhs.org/preserve/conservation/reports/nytimes_preserving.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Getting Started with Microfilm:
&lt;a href=&quot;http://www.archives.state.al.us/officials/microfilmbasics02.html&quot;&gt;http://www.archives.state.al.us/officials/microfilmbasics02.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A Word About Microhistory:
&lt;a href=&quot;http://historynewsnetwork.org/article/23720&quot;&gt;http://historynewsnetwork.org/article/23720&lt;/a&gt;&lt;/p&gt;
</description>
				<guid isPermaLink="true">/technical/-/curiosities/cyr-WhenAPatchWontDo/</guid>
			</item>
		
			<item>
				<title>“Avoid Female Society”</title>
				<link>/curiosities/black-avoid-female-society/</link>
				<pubDate>Sun, 06 Nov 2016 00:00:00 -0400</pubDate>
				<description>&lt;p&gt;The &lt;em&gt;Egyptian Gazette&lt;/em&gt; includes a section called &lt;strong&gt;“CALENDAR OF THE WEEK”&lt;/strong&gt; in every issue which lists the important events happening in Alexandria and its surrounding areas. This serves as a tool to keep its citizens up-to-date with what is happening in the town and advises them to go and participate in these events. Throughout the week of August 7 to August 12, 1905, the section lists plays, concerts, and other kinds of events for general entertainment. However, the issue for August 7, 1905 includes a peculiar calendar of events. For the areas of Coptic and Mohamedan, the calendar lists actions the people should take rather than events they should attend. What is even more interesting is the actions themselves. For Sunday, August 6th, it reads, “Avoid female society” and on Tuesday, August 8, it reads “Avoid eating onions and garlic”. The week gets even more interesting by following with “Drink cold water before breakfast” and “Great abundance of watermelons”. The calendar section leaves no explanation as to why these events have been posted. I am curious to know whether it was some kind of joke or if these events were meant to be taken seriously for some kind of reason.&lt;/p&gt;
</description>
				<guid isPermaLink="true">/curiosities/black-avoid-female-society/</guid>
			</item>
		
			<item>
				<title>Clean Killer</title>
				<link>/curiosities/Soap/</link>
				<pubDate>Sun, 06 Nov 2016 00:00:00 -0400</pubDate>
				<description>&lt;p&gt;During a time when merchandise regulation is not nearly as controlled as it is now, it can be expected that there will be defaults in the products produced. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2220712/?page=1 Define Salicylate of Soda&amp;quot;&quot;&gt;Salicylate of soda&lt;/a&gt; was thought to have restorative effects during the mid to late 19th century. It was often recommended to people with joint disease, such as rheumatism, to alleviate pain.  At first the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1322509/?page=7&quot;&gt;negative side effects&lt;/a&gt; associated with salicylate of soda reported were minor, like a relapse in pain after the salicylate of soda was no longer being used. Many users had to be gradually weaned from the chemical.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/dig-eg-gaz/dig-eg-gaz.github.io/blob/master/images/blog-images/soap-ad.png?raw=true&quot; alt=&quot;Sunlight Soap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A few years after its use in soap, physicians began noting that people who were repeated salicylate of soda users had painful skin and there was even a case of failing vision after taking a bath in the soap with the chemicals. In the early 1900s, manufacturers of soap products containing the chemical were ordered to stop production and were prosecuted by the &lt;a href=&quot;https://books.google.com/books?id=5a-Uc3NJuPUC&amp;amp;pg=PA206&amp;amp;lpg=PA206&amp;amp;dq=1905+egyptian+sanitary+administration&amp;amp;source=bl&amp;amp;ots=pGXkko-vu3&amp;amp;sig=P6WCUO4P1BmN_u7Zm8Ny2nSY3Dc&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwi60p_rs4zQAhWSZiYKHXwdDTMQ6AEIHTAA#v=onepage&amp;amp;q=1905%20egyptian%20sanitary%20administration&amp;amp;f=false&quot;&gt;sanitary administration&lt;/a&gt;. Large soap companies during this time, like Pears’ Soap, were not being advertised because of the ordeal. Instead, the same issue of the newspaper that published the harmful soap announcement had an ad for Sunlight Soap, which advertises its lack of “obnoxious or harmful feature.” This small news brief gives insight about this time period, showing its lack of medical knowledge and its clever marketing methods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/dig-eg-gaz/dig-eg-gaz.github.io/blob/master/images/blog-images/pear-soap.png?raw=true&quot; alt=&quot;Pear Soap&quot; /&gt;&lt;/p&gt;
</description>
				<guid isPermaLink="true">/curiosities/Soap/</guid>
			</item>
		
			<item>
				<title>Haskell and XML</title>
				<link>/technical/Haskell-and-XML/</link>
				<pubDate>Sun, 06 Nov 2016 00:00:00 -0400</pubDate>
				<description>&lt;p&gt;I got pretty bored performing the same search-and-replace actions for a variety of different place names.  As any programmer knows, performing the same actions over and over again signals a perfect opportunity to use code to script out those actions.&lt;/p&gt;

&lt;p&gt;I spent time this weekend investigating the possibility of using Haskell to write a script to automatically TEI index certain words with tags and reference data.&lt;/p&gt;

&lt;p&gt;The program (which would be a command-line utility at first) needs to perform a few basic functions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Add tags to text inside any xml nodes&lt;/li&gt;
  &lt;li&gt;Update attributes of tags with specified contents&lt;/li&gt;
  &lt;li&gt;Take in input file that describes a set of commands&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This would potentially allow me to update every xml file in the entire repository simultaneously, inserting the proper tags for a list of names, dates, and locations.  Something like:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yaml
placename:
  - (Tokyo, ref, some-link-to-wikidata)
  - (London, ref, some-link-to-wikidata)
  - (St. Petersburg, ref, some-link-to-wikidata)
  - (Port Said, ref, some-link-to-wikidata)
persname:
  - (Lord Cromer, ref, some-link-to-person-data)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This would allow me to just keep an up-to-date set of location names, attribute names, and links that I could then very easily update or apply to any issue in the entire year, and perhaps even set it to run every time I changed a file, so I would never have to worry about manually changing this information.&lt;/p&gt;

&lt;h2 id=&quot;a-quick-note-on-xml&quot;&gt;A quick note on XML&lt;/h2&gt;
&lt;p&gt;XML can be visualized as a “rose tree” made of nodes.  This basically means that every part of the XML document is some sort of node, which has other nodes inside of it.  The nodes can be any type of tag, or any plain text, and any number of both in any order.&lt;/p&gt;

&lt;p&gt;Here’s what some XML in your issue might look like as a Tree&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
div -*- dateline -*- date - Text
     |            *- Text
     |            *- placename - Text
     |
     *- p         - Text
     *- byline    - Text
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And here it is as XML&lt;/p&gt;

&lt;p&gt;```XML&lt;/p&gt;
&lt;div&gt;
    &lt;dateline&gt;&lt;date&gt;2nd October 1905&lt;/date&gt;, &lt;placename&gt;Port Said&lt;/placename&gt;&lt;/dateline&gt;
    &lt;p&gt;Here is some text, detailing an amazing story about something.&lt;/p&gt;
    &lt;byline&gt;(Reuter.)&lt;/byline&gt;
&lt;/div&gt;
&lt;p&gt;```&lt;/p&gt;

&lt;h2 id=&quot;functional-programming&quot;&gt;Functional Programming&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://www.haskell.org/static/img/haskell-logo.svg?etag=ukf3Fg7-&quot; alt=&quot;Haskell Logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I program in Haskell mostly.  Haskell is a pure functional language, which just means it’s not the same as the traditional programming languages you see day-to-day, and it’s not something FSU teaches currently.&lt;/p&gt;

&lt;p&gt;It looks something like this:&lt;/p&gt;

&lt;p&gt;```Haskell&lt;/p&gt;

&lt;p&gt;and :: Bool -&amp;gt; Bool -&amp;gt; Bool
and c t = if c then t else False&lt;/p&gt;

&lt;p&gt;filter :: (a -&amp;gt; Bool) -&amp;gt; [a] -&amp;gt; [a]
filter p xs = [x | x &amp;lt;- xs, p x]&lt;/p&gt;

&lt;p&gt;map :: (a -&amp;gt; b) -&amp;gt; [a] -&amp;gt; [b]
map _ []   = []
map f x:xs = f x : map xs&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;It’s pretty rad, it looks quite unlike most languages I’ve seen in my day.  Haskell has some really interesting properties, though.  It’s really easy to parse documents as a result, but it’s difficult to transform them easily.  Many libraries exist to extract data easily and reliably from xml documents, but few to transform or modify them in a programatic way.&lt;/p&gt;

&lt;p&gt;I did find one eventually: the &lt;a href=&quot;https://wiki.haskell.org/HXT&quot;&gt;Haskell XML Toolbox&lt;/a&gt;.  It lets you do some pretty interesting things.  It uses something called arrows (a &lt;em&gt;very&lt;/em&gt; trippy functional programming concept), to allow the programmer to compose filters together to navigate the XML tree.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://wiki.haskell.org/HXT#The_concept_of_filters&quot;&gt;documentation&lt;/a&gt; has this example:&lt;/p&gt;

&lt;p&gt;```Haskell&lt;/p&gt;

&lt;p&gt;isA  :: (a -&amp;gt; Bool) -&amp;gt; (a -&amp;gt; [a])
isA p x | p x       = [x]
        | otherwise = []&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;It takes a function (called a predicate) that returns a True/False value, and creates a new function that can process, or filter, values.  Essentially, these arrows let us process parts of a tree into no results, one result, or many results.  By using different types of filters, and by putting these filters together, with some relatively simple structure we can create some &lt;em&gt;extremely&lt;/em&gt; complex instructions to parse and transform the document.&lt;/p&gt;

&lt;p&gt;Again, an example from the documentation:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Haskell
getGrandChildren :: XmlFilter
getGrandChildren = getChildren &amp;gt;&amp;gt;&amp;gt; getChildren
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;(&amp;gt;&amp;gt;&amp;gt;)&lt;/code&gt; operator makes it extremely trivial to chain filters together to make new filters.  The ability to compose functions like this is the hallmark of functional programming, and works extremely well in this case.&lt;/p&gt;

&lt;p&gt;By defining a few more combination operators, the HXT library quickly builds up sufficient capability to tackle some really complicated parsing, as in:&lt;/p&gt;

&lt;p&gt;```Haskell&lt;/p&gt;

&lt;p&gt;getTextChildren2 :: XmlFilter
getTextChildren2 = getChildren »&amp;gt; ( isXText &amp;lt;+&amp;gt; ( getChildren »&amp;gt; isXText ) )&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;which only gets the text children for of the current tag, and the text of any tags inside the current one.&lt;/p&gt;

&lt;p&gt;Using this extremely flexible library, I hope to find a practical way to automate much of the task of TEI indexing the Egyptian Gazette!&lt;/p&gt;
</description>
				<guid isPermaLink="true">/technical/Haskell-and-XML/</guid>
			</item>
		
			<item>
				<title>Bubonic Plague In Egypt</title>
				<link>/curiosities/-/Cyr-Bubonic-Plague/</link>
				<pubDate>Sun, 06 Nov 2016 00:00:00 -0400</pubDate>
				<description>&lt;p&gt;According to the Egyptian Gazette, August 28, 1905, the Bubonic plague has reported at least thirteen cases of individuals that had contracted this disease.&lt;/p&gt;

&lt;p&gt;Interest in this snippet of information, housed within the Gazette (see figure below), stems from the fact that the Bubonic Plague (a.k.a. The Black Death), was not confined to Medieval Europe (as many readers would suspect). In fact, the Bubonic plague has existed for thousands of years, the first known case being recorded in China dating back to 224 B.C.E..&lt;/p&gt;

&lt;p&gt;The most significant outbreak, and most noticeable, occurred in Europe in the mid-fourteenth century over a five year period (1347 to 1352). During this time period, sources indicate that at least 25 million people died, about one-third of the continent’s population (these numbers are disputed). The often disputed claims as to the cause of such a disease was further investigated in 2010 and 2011, when researchers analyzed DNA from some of its European victims. The consensus to date is that the pathogen responsible was the Yersinia pestis bacterium, which most likely caused several different forms of the plague.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/dig-eg-gaz/dig-eg-gaz.github.io/blob/master/images/blog-images/Black-Plague.png&quot; alt=&quot;image name&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Seeing an outbreak within the borders of Egypt, with so few deaths was incredible, which warrants further understanding of this disease, how it was spread and who it affected. The Gazette’s article (above) mentions that approximately 13 cases were under treatment during the middle of August and beginning of September, 1905. With the plague’s notorious reputation, the Gazette’s allocated space to this pandemic is sapient, especially due to the many cattle deaths in the area within that same time period (the figure to the right indicates the area affected by the Bubonic Plague).&lt;/p&gt;

&lt;p&gt;With modern populations shifted to cities in modern day, an unidentifiable plague in one of these areas would prove to be even more catastrophic than ever before.&lt;/p&gt;

&lt;h2 id=&quot;for-further-study&quot;&gt;For further study:&lt;/h2&gt;
&lt;p&gt;Yersinia Pestis.
&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2951374/&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2951374/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A Draft genome of Yersinia pestis from victims of the Black Death.
&lt;a href=&quot;https://www.nature.com/nature/journal/v478/n7370/full/nature10549.html&quot;&gt;https://www.nature.com/nature/journal/v478/n7370/full/nature10549.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Plague in 18th Century Egypt.
&lt;a href=&quot;https://contagions.wordpress.com/2010/05/26/plague-in-18th-century-egypt/&quot;&gt;https://contagions.wordpress.com/2010/05/26/plague-in-18th-century-egypt/&lt;/a&gt;&lt;/p&gt;
</description>
				<guid isPermaLink="true">/curiosities/-/Cyr-Bubonic-Plague/</guid>
			</item>
		
			<item>
				<title>XML-izing</title>
				<link>/technical/XML-izing/</link>
				<pubDate>Tue, 01 Nov 2016 00:00:00 -0400</pubDate>
				<description>&lt;p&gt;I can’t say that around fifty hours of work in Oxygen was what I expected when I signed up for this class, but it’s certainly what I’ve been given. Work in Oxygen can be rewarding, tedious, and frustrating in one package, usually at the same time. While I have sympathies for my classmates who saw massive technical issues, I never
had anything that problematic when I was XML-izing my files.&lt;/p&gt;

&lt;p&gt;The most difficult part of this class was largely typing most of my week by hand. FineReader’s trial ran out on my desktop, and when I tried to download it on my Mac, I found that it didn’t work anywhere near as well as it did on my desktop. So I would up typing the entirety of my week under my own manpower, rather than relying on AABBY.&lt;/p&gt;

&lt;p&gt;Beyond that, it was a fairly methodical, sensible process. There were instances of annoyance, like when I would accidentally delete a div and the entire thing would collapse, but I’m pretty sure that part’s past me now. I’m looking forward to going more in-depth into the Gazette.&lt;/p&gt;
</description>
				<guid isPermaLink="true">/technical/XML-izing/</guid>
			</item>
		
	</channel>
</rss>
